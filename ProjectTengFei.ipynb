{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tabula.io import read_pdf\n",
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. look for Tesla's high freq. terms and check the difference compared to NIO、XPeng、Li"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "def show_ner_with_page(text: str):\n",
    "    '''\n",
    "    This function requires nlp preloaded\n",
    "    :param text: the text\n",
    "    :return: returns a display dataframe with all the entity included\n",
    "    '''\n",
    "    display_df = pd.DataFrame()\n",
    "    if nlp(text).ents:\n",
    "        for ent in nlp(text).ents:\n",
    "            val = [ent.text, ent.start_char, ent.end_char, ent.label_]\n",
    "            df = pd.DataFrame(val).transpose()\n",
    "            display_df = display_df.append(df, ignore_index=True)\n",
    "\n",
    "    display_df.columns = ['entities', 'start_char', 'end_char', 'label']\n",
    "    return display_df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def testGetter(file: str, cut: int):\n",
    "    '''\n",
    "    financial report is too big, we can use a few pages for the test.\n",
    "    for example, if cut is 100, then we only get the first 1% pages of the file.\n",
    "    :param file: input the file name\n",
    "    :return: the output is a part of text file\n",
    "    '''\n",
    "    f = open(file, 'r')\n",
    "    text = f.read()\n",
    "    leng = int(len(text) / cut)\n",
    "\n",
    "    return text[:leng].replace('\\n', '')\n",
    "# testGetter('Li_AnnualReport_2020.txt', 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead. [3619817759.py:13]\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                      entities start_char end_char     label\n0                                         D.C.         76       80       GPE\n1                                           20         91       93  CARDINAL\n2              REGISTRATION STATEMENT PURSUANT        107      138    PERSON\n3    1934For the fiscal year ended December 31        286      327      DATE\n4                                      1934For        421      428  CARDINAL\n..                                         ...        ...      ...       ...\n140                                       H.10       9600     9604       GPE\n141                     The Board of Governors       9628     9650       ORG\n142                 the Federal Reserve System       9654     9680       ORG\n143                                       U.S.       9774     9778       GPE\n144                                     annual       9891     9897      DATE\n\n[145 rows x 4 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>entities</th>\n      <th>start_char</th>\n      <th>end_char</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>D.C.</td>\n      <td>76</td>\n      <td>80</td>\n      <td>GPE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20</td>\n      <td>91</td>\n      <td>93</td>\n      <td>CARDINAL</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>REGISTRATION STATEMENT PURSUANT</td>\n      <td>107</td>\n      <td>138</td>\n      <td>PERSON</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1934For the fiscal year ended December 31</td>\n      <td>286</td>\n      <td>327</td>\n      <td>DATE</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1934For</td>\n      <td>421</td>\n      <td>428</td>\n      <td>CARDINAL</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>140</th>\n      <td>H.10</td>\n      <td>9600</td>\n      <td>9604</td>\n      <td>GPE</td>\n    </tr>\n    <tr>\n      <th>141</th>\n      <td>The Board of Governors</td>\n      <td>9628</td>\n      <td>9650</td>\n      <td>ORG</td>\n    </tr>\n    <tr>\n      <th>142</th>\n      <td>the Federal Reserve System</td>\n      <td>9654</td>\n      <td>9680</td>\n      <td>ORG</td>\n    </tr>\n    <tr>\n      <th>143</th>\n      <td>U.S.</td>\n      <td>9774</td>\n      <td>9778</td>\n      <td>GPE</td>\n    </tr>\n    <tr>\n      <th>144</th>\n      <td>annual</td>\n      <td>9891</td>\n      <td>9897</td>\n      <td>DATE</td>\n    </tr>\n  </tbody>\n</table>\n<p>145 rows × 4 columns</p>\n</div>"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this line shows the NER\n",
    "show_ner_with_page(testGetter('Report/Li_AnnualReport_2020.txt', cut=100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 2000000\n",
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "\n",
    "    def set_stopwords(self, stopwords):\n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "\n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "\n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "\n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "\n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "\n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "\n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "\n",
    "        return g_norm\n",
    "\n",
    "\n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        res = []\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            # print(key + ' - ' + str(value))\n",
    "            res.append([key, str(value)])\n",
    "            if i > number:\n",
    "                break\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "    def analyze(self, text,\n",
    "                candidate_pos=['NOUN', 'PROPN'],\n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "\n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "\n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "\n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "\n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "\n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "\n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "\n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "\n",
    "        self.node_weight = node_weight"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [],
   "source": [
    "def keywordGetter(filepath: str, file_cut: int,  candidate_pos=['NOUN', 'PROPN'], numOfKeywords=50):\n",
    "    if file_cut == 0:\n",
    "        # we'll use the whole file\n",
    "        f = open(filepath, 'r')\n",
    "        text4Rank = f.read()\n",
    "        f.close()\n",
    "        tr4w = TextRank4Keyword()\n",
    "        tr4w.analyze(text4Rank, candidate_pos=candidate_pos, window_size=4, lower=False)\n",
    "    else:\n",
    "        # test mode\n",
    "        text4Rank = testGetter(filepath, file_cut)\n",
    "        tr4w = TextRank4Keyword()\n",
    "        tr4w.analyze(text4Rank, candidate_pos=candidate_pos, window_size=4, lower=False)\n",
    "\n",
    "    res = tr4w.get_keywords(numOfKeywords)\n",
    "\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "# this cell will take about 10 min to run\n",
    "keyword_Tesla = keywordGetter('Report/Tesla_AnnualReport_2020.txt', 0)\n",
    "keyword_Li = keywordGetter('Report/Li_AnnualReport_2020.txt', 0)\n",
    "keyword_NIO = keywordGetter('Report/NIO_AnnualReport_2020.txt', 0)\n",
    "keyword_XPeng = keywordGetter('Report/XPeng_AnnualReport_2020.txt', 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [],
   "source": [
    "def string_compare(firmA, firmB):\n",
    "\n",
    "    shared = []\n",
    "    keywordA = [x[0] for x in firmA]\n",
    "    keywordB = [x[0] for x in firmB]\n",
    "    for word in keywordA:\n",
    "        if word in keywordB:\n",
    "            shared.append(word)\n",
    "\n",
    "    temp = set(keywordA) | set(keywordB)\n",
    "    difference = list(temp - set(shared))\n",
    "\n",
    "    return [shared, difference]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "data": {
      "text/plain": "['year',\n 'energy',\n 'securities',\n 'China',\n 'lease',\n 'period',\n 'stock',\n 'Model',\n 'Solar',\n 'expenses',\n 'shareholders',\n 'products',\n 'service',\n 'tsla-20211231.htm',\n 'Shares',\n 'Contents',\n 'Table',\n 'requirements',\n 'edgar',\n 'share',\n 'rights',\n 'cost',\n 'Li',\n 'Gigafactory',\n 'system',\n 'Company',\n 'production',\n 'revenue',\n 'storage',\n 'equity',\n 'systems',\n 'costs',\n 'years',\n 'PRC',\n 'cash',\n 'January',\n 'Group',\n 'loss',\n 'management',\n 'debt',\n 'Party',\n 'shares',\n 'date',\n 'company',\n 'operating',\n 'regulations',\n 'Tesla',\n 'Ñ',\n 'development',\n 'users',\n 'laws',\n 'Class',\n 'ADSs',\n 'price',\n 'Agreement',\n 'battery',\n 'manufacturing',\n 'customers',\n 'liabilities',\n 'Beijing',\n 'Archives',\n 'respect',\n 'February',\n 'statements']"
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string_compare(keyword_Tesla, keyword_Li)[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "with open('string_compare.txt', 'w') as f:\n",
    "    for firm in ['Li', 'NIO', 'XPeng']:\n",
    "        f.write(firm + '\\n')\n",
    "        f.write('  ' + 'shared' + '\\n')\n",
    "\n",
    "        if firm == 'Li':\n",
    "            temp = string_compare(keyword_Tesla, keyword_Li)\n",
    "            f.write(' '.join(x for x in temp[0]))\n",
    "        if firm == 'NIO':\n",
    "            temp = string_compare(keyword_Tesla, keyword_NIO)\n",
    "            f.write(' '.join(x for x in temp[0]))\n",
    "        if firm == 'XPeng':\n",
    "            temp = string_compare(keyword_Tesla, keyword_XPeng)\n",
    "            f.write(' '.join(x for x in temp[0]))\n",
    "\n",
    "        f.write('\\n  ' + 'difference' + '\\n')\n",
    "\n",
    "        if firm == 'Li':\n",
    "            temp = string_compare(keyword_Tesla, keyword_Li)\n",
    "            f.write(' '.join(x for x in temp[1]))\n",
    "        if firm == 'NIO':\n",
    "            temp = string_compare(keyword_Tesla, keyword_NIO)\n",
    "            f.write(' '.join(x for x in temp[1]))\n",
    "        if firm == 'XPeng':\n",
    "            temp = string_compare(keyword_Tesla, keyword_XPeng)\n",
    "            f.write(' '.join(x for x in temp[1]))\n",
    "\n",
    "        f.write('\\n\\n\\n')\n",
    "    f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Relation graph for these four new energy car manufacturers.\n",
    "The graphs are stored in html file in folder `news_graph`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "import os\n",
    "sys.path.insert(0, '/Users/feiteng/Documents/MGTF_venv/FinalProj/news_graph')\n",
    "from news_graph.news_graph import NewsMining"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "outputs": [],
   "source": [
    "# test\n",
    "content = testGetter('Report/Li_AnnualReport_2020.txt', cut=100)\n",
    "# whole text\n",
    "# f = open('Report/Li_AnnualReport_2020.txt', 'r')\n",
    "# content = f.read()\n",
    "# f.close()\n",
    "Miner = NewsMining()\n",
    "Miner.main(content)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "mgtf_venv",
   "language": "python",
   "display_name": "MGTF_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}